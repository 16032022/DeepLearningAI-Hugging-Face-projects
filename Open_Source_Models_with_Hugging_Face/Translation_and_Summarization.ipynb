{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6685771",
   "metadata": {},
   "source": [
    "# Translation and Summarization 🎯 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd303ad9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to use the Hugging Face Transformers library for `translation and summarization` tasks. We will go through the steps of setting up the environment, preparing the data, building the pipelines, and evaluating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff2fb1",
   "metadata": {},
   "source": [
    "### Install and update the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c014cb4",
   "metadata": {},
   "source": [
    "Here is a brief description of the required libraries:\n",
    "\n",
    "- The transformers library by Hugging Face provides state-of-the-art pre-trained models for natural language processing (NLP) tasks such as text classification, translation, summarization, and more. \n",
    "\n",
    "- The pipeline function from the transformers library by Hugging Face is used to easily access pre-trained models for various natural language processing (NLP) tasks. By calling pipeline, you can quickly load models for tasks like sentiment analysis, text generation, translation, question answering, and more.\n",
    "\n",
    "- PyTorch is an open-source deep learning framework developed by Facebook's AI Research lab. It provides flexibility and speed for building, training, and deploying deep learning models. \n",
    "\n",
    "- The textwrap library in Python provides utilities for formatting and wrapping plain text. It helps in breaking long strings into lines of a specified width, making the text more readable. This is particularly useful for generating neatly formatted text outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c5dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.1 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782af222-1bea-449a-8dd4-655ad7a7b8ea",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Suppress warning messages such as non-critical log messages\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9449fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea43ec1",
   "metadata": {},
   "source": [
    "### Build the `translation` pipeline using 🤗 Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87460089",
   "metadata": {},
   "source": [
    "Build and train the translation pipeline using the Transformers library.\n",
    "\n",
    "### Load Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8e7ed",
   "metadata": {},
   "source": [
    "The NLLB (No Language Left Behind) model has been selected since it offers significant benefits for translation tasks. Info about NLLB  ['nllb-200-distilled-600M'](https://huggingface.co/facebook/nllb-200-distilled-600M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad4b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the translation pipeline using the NLLB model\n",
    "translator = pipeline(task=\"translation\", \n",
    "                      model=\"facebook/nllb-200-distilled-600M\", \n",
    "                      torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbf879",
   "metadata": {},
   "source": [
    "### Translation Examples\n",
    "Perform translation on example texts.\n",
    "\n",
    " - Example 1: Translation from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095bd1c5-a96f-4b20-8e9c-601b0b158fd8",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text = \"\"\"\\\n",
    "My puppy is adorable, \\\n",
    "Your kitten is cute.\n",
    "Her panda is friendly.\n",
    "His llama is thoughtful. \\\n",
    "We all have nice pets!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75575792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My puppy is adorable, Your kitten is cute.\n",
      "Her panda is friendly.\n",
      "His llama is thoughtful. We all have nice pets!\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28447cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the given text is:\n",
      "Mon chiot est adorable, ton chaton est mignon, son\n",
      "panda est ami, sa lamme est attentive, nous avons\n",
      "tous de beaux animaux de compagnie.\n"
     ]
    }
   ],
   "source": [
    "# Translate the input text from English to French\n",
    "text_translated = translator(text,\n",
    "                             src_lang=\"eng_Latn\",\n",
    "                             tgt_lang=\"fra_Latn\")\n",
    "\n",
    "# Uncomment the following line to print the text_translated\n",
    "#text_translated\n",
    "\n",
    "# Extract and print the translation text\n",
    "translation_text = text_translated[0]['translation_text']\n",
    "wrapped_text = textwrap.fill(translation_text, width=50)\n",
    "print(f\"The translation of the given text is:\\n{wrapped_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ab814",
   "metadata": {},
   "source": [
    "To choose other languages, see more info on the page: [Languages in FLORES-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2307748",
   "metadata": {},
   "source": [
    "- Example 2: Translation from English to Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6e3e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are you doing today?\n",
      "\n",
      "The translation of the given text is:\n",
      "Wat doe je vandaag?\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "text1 = \"\"\"\n",
    "What are you doing today?\n",
    "\"\"\"\n",
    "print(text1)\n",
    "\n",
    "# Translate the input text from English to Dutch\n",
    "text_translated1 = translator(text1, src_lang=\"eng_Latn\", tgt_lang=\"nld_Latn\")  # Dutch\n",
    "\n",
    "# Uncomment the following line to print the text_translated\n",
    "#text_translated1\n",
    "\n",
    "# Extract and print the translation text\n",
    "translation_text = text_translated1[0]['translation_text']\n",
    "wrapped_text = textwrap.fill(translation_text, width=50)\n",
    "print(f\"The translation of the given text is:\\n{wrapped_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7517649",
   "metadata": {},
   "source": [
    "- Free up some memory before continuing with the summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247ca761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the translator model and free up memory\n",
    "del translator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fac55f",
   "metadata": {},
   "source": [
    "### Build the `summarization` pipeline using 🤗 Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca7415",
   "metadata": {},
   "source": [
    "Summarization Pipeline\n",
    "Build and train the summarization pipeline using the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f173f8",
   "metadata": {},
   "source": [
    "The BART (Bidirectional and Auto-Regressive Transformers) model has been choosen since it is highly effective for abstractive summarization and other NLP tasks. More info ['bart-large-cnn'](https://huggingface.co/facebook/bart-large-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab73a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summarization pipeline using the BART model\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"facebook/bart-large-cnn\",\n",
    "                      torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee91b8",
   "metadata": {},
   "source": [
    "### Summarization Examples\n",
    "Perform summarization on example texts.\n",
    "\n",
    " - Example 1: Summarization of a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98276d66-4274-4a2f-b6a7-b4fb839b94f7",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text = \"\"\"Paris is the capital and most populous city of France, with\n",
    "          an estimated population of 2,175,601 residents as of 2018,\n",
    "          in an area of more than 105 square kilometres (41 square\n",
    "          miles). The City of Paris is the centre and seat of\n",
    "          government of the region and province of Île-de-France, or\n",
    "          Paris Region, which has an estimated population of\n",
    "          12,174,880, or about 18 percent of the population of France\n",
    "          as of 2017.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d856f193-cbf7-450b-8ae3-42287096e56f",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Paris is the capital and most populous city of\n",
      "France, with an estimated population of 2,175,601\n",
      "residents as of 2018. The City of Paris is the\n",
      "centre and seat of the government of the region\n",
      "and province of Île-de-France.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the input text\n",
    "summary = summarizer(text,\n",
    "                     min_length=10,\n",
    "                     max_length=100)\n",
    "\n",
    "# Uncomment the following line to print the summary\n",
    "#summary\n",
    "\n",
    "# Extract and print the summary text\n",
    "summary_text = summary[0]['summary_text']\n",
    "wrapped_text = textwrap.fill(summary_text, width=50)\n",
    "print(f\"Summary:\\n{wrapped_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b520ba",
   "metadata": {},
   "source": [
    "- Example 2: Summarization of a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "002c0464-7d37-4b50-b3fe-4099101e2c8b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text2 = \"\"\"Amsterdam, the capital of the Netherlands, is known for its picturesque canals,\n",
    "        historic architecture, and vibrant cultural scene. Founded in the 12th century as \n",
    "        a small fishing village, it grew into a major global trading hub during the Dutch Golden \n",
    "        Age. Today, Amsterdam is a cosmopolitan city, famous for its rich artistic heritage, \n",
    "        with museums like the Van Gogh Museum and Rijksmuseum. The city is also known for its \n",
    "        liberal attitudes, such as tolerance of cannabis use and legal sex work, as well \n",
    "        as its cycling culture, eco-friendly initiatives, and diverse, international population.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e67790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amsterdam, the capital of the Netherlands, is\n",
      "known for its picturesque canals, historic\n",
      "architecture, and vibrant cultural scene. Founded\n",
      "in the 12th century as a small fishing village, it\n",
      "grew into a major global trading hub. Today,\n",
      "Amsterdam is a cosmopolitan city, famous for its\n",
      "rich artistic heritage, with museums like the Van\n",
      "Gogh Museum and Rijksmuseum.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the input text\n",
    "summary2 = summarizer(text2,\n",
    "                     min_length=10,\n",
    "                     max_length=100)\n",
    "\n",
    "# Uncomment the following line to print the summary2\n",
    "#summary2\n",
    "\n",
    "# Extract and print the summary text\n",
    "summary_text = summary2[0]['summary_text']\n",
    "wrapped_text = textwrap.fill(summary_text, width=50)\n",
    "print(f\"Summary:\\n{wrapped_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9aaa3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This notebook demonstrated how to use the Hugging Face Transformers library for translation and summarization tasks.\n",
    "- The NLLB model is valuable for translating low-resource languages but has challenges. It supports inclusivity, delivers quality translations, and is open-source. However, it may carry biases, struggle with underrepresented languages, and lack documentation. It suits research projects but is less ideal for specialized translations like medical or legal texts.\n",
    "\n",
    "- The BART model excels at abstractive summarization and other NLP tasks, offering high-quality, flexible, and customizable results backed by an active community. However, its high computational demands, hallucination risk, and struggles with domain-specific or poorly structured inputs are limitations. BART works best for summarizing moderately structured inputs like news articles or reports. It is less suited for unstructured inputs like raw comments or fragmented notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4a0ba",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "- Try this model with your own texts!\n",
    "- Experiment with different models and parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
