{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5753a0c",
   "metadata": {},
   "source": [
    "# Chat with any LLM! ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d115b64",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build an application to chat with any Large Language Model (LLM) using Gradio. We will leverage the [Hugging Face API](https://huggingface.co) to interact with the `falcon-40b-instruct` model, which is one of the top-ranking open-source LLMs. By the end of this notebook, you will have a functional chat application that can interact with users using natural language.\n",
    "\n",
    "- Key steps include:\n",
    "1. Setting up the environment and installing necessary packages.\n",
    "2. Loading the LLM and configuring the API.\n",
    "3. Building the Gradio interface for user interaction.\n",
    "4. Adding advanced features like chat history and customized prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b5629",
   "metadata": {},
   "source": [
    "### Setting up the environment and installing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ce94c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: text-generation in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: gradio in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.10.0)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from text-generation) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from text-generation) (0.27.1)\n",
      "Requirement already satisfied: pydantic<3,>2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from text-generation) (2.10.4)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.115.6)\n",
      "Requirement already satisfied: ffmpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.5.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (3.10.13)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydub in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.8.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.41.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from gradio-client==1.5.3->gradio) (2024.2.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gradio-client==1.5.3->gradio) (14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0,>=3.8->text-generation) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.12->text-generation) (3.13.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.12->text-generation) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.12->text-generation) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>2->text-generation) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>2->text-generation) (2.27.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.12->text-generation) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.12->text-generation) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install python-dotenv text-generation gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50593af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import requests\n",
    "import warnings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "from text_generation import Client\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "hf_api_key = os.environ.get('HF_API_KEY')\n",
    "hf_api_falcom_base = os.environ.get('HF_API_FALCOM_BASE')\n",
    "\n",
    "# Uncomment the following line to print HF API Key and hf_api_falcom_base\n",
    "#print(\"HF API Key:\", hf_api_key)\n",
    "#print(\"Endpoint URL:\", hf_api_falcom_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411fae46",
   "metadata": {},
   "source": [
    "To configure the text-generation model, the correct endpoint (HF_API_FALCOM_BASE) must be selected, connecting the application to the desired model. Initially, the falcon-40b-instruct model [Inference Endpoint](https://huggingface.co/inference-endpoints) was suggested, but its Inference API has been deactivated. As a result, the tiiuae/falcon-7b-instruct model was choosen due to its similar capabilities, efficient performance, and lower resource requirements. This model remains effective for text-generation tasks, making it a practical choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55a5d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation client\n",
    "client = Client(hf_api_falcom_base, headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d854e7",
   "metadata": {},
   "source": [
    "- 1 method : Suggestion from Github copilot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f127813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3b1a945758ed97c56f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3b1a945758ed97c56f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the chat function\n",
    "def generate(input, max_new_tokens):\n",
    "    try:\n",
    "        response = client.generate(input, max_new_tokens=max_new_tokens)\n",
    "        return response.generated_text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "        gr.Slider(label=\"Max new tokens\", value=20, maximum=1024, minimum=1)\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Completion\")\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True, server_port=int(os.environ.get('PORT1', 7860)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59b9dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6fc97",
   "metadata": {},
   "source": [
    "## Building an app to chat with any LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a3c9b",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , the best ranking open source LLM on the [ðŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Nowdays, the `falcon-40b-instruct` Inference API has been deactivated. As a result, the tiiuae/falcon-7b-instruct model was choosen due to its similar capabilities, efficient performance, and lower resource requirements. This model remains effective for text-generation tasks, making it a practical choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c32879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_text='\\nMath has been discovered, not invented. It is a system of rules and formulas that are used to describe the natural world and its behavior.' details=Details(finish_reason=<FinishReason.EndOfSequenceToken: 'eos_token'>, generated_tokens=30, seed=None, prefill=[], tokens=[Token(id=193, text='\\n', logprob=-0.004764557, special=False), Token(id=25864, text='Math', logprob=-0.19885254, special=False), Token(id=504, text=' has', logprob=-0.3696289, special=False), Token(id=650, text=' been', logprob=-0.6875, special=False), Token(id=6524, text=' discovered', logprob=-0.5136719, special=False), Token(id=23, text=',', logprob=-1.4863281, special=False), Token(id=416, text=' not', logprob=-0.79785156, special=False), Token(id=21886, text=' invented', logprob=-0.0031585693, special=False), Token(id=25, text='.', logprob=-0.09484863, special=False), Token(id=605, text=' It', logprob=-1.2431641, special=False), Token(id=304, text=' is', logprob=-0.93408203, special=False), Token(id=241, text=' a', logprob=-0.7324219, special=False), Token(id=1092, text=' system', logprob=-1.8076172, special=False), Token(id=275, text=' of', logprob=-0.08630371, special=False), Token(id=4213, text=' rules', logprob=-1.8759766, special=False), Token(id=273, text=' and', logprob=-0.48168945, special=False), Token(id=29418, text=' formulas', logprob=-1.7167969, special=False), Token(id=325, text=' that', logprob=-0.47973633, special=False), Token(id=362, text=' are', logprob=-1.4433594, special=False), Token(id=1042, text=' used', logprob=-0.2626953, special=False), Token(id=271, text=' to', logprob=-0.13977051, special=False), Token(id=7474, text=' describe', logprob=-1.5380859, special=False), Token(id=248, text=' the', logprob=-0.6147461, special=False), Token(id=2616, text=' natural', logprob=-0.8886719, special=False), Token(id=1079, text=' world', logprob=-0.053009033, special=False), Token(id=273, text=' and', logprob=-0.5957031, special=False), Token(id=701, text=' its', logprob=-0.8833008, special=False), Token(id=5188, text=' behavior', logprob=-1.6728516, special=False), Token(id=25, text='.', logprob=-0.16625977, special=False), Token(id=11, text='<|endoftext|>', logprob=-1.4736328, special=True)], top_tokens=None, best_of_sequences=None)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "response=client.generate(prompt, max_new_tokens=256)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a907b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation client\n",
    "client = Client(hf_api_falcom_base, headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://983b3a5daccd2629d7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://983b3a5daccd2629d7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import gradio as gr\n",
    "def generate(input, slider):\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "                            gr.Slider(label=\"Max new tokens\", \n",
    "                                      value=20,  \n",
    "                                      maximum=1024, \n",
    "                                      minimum=1)], \n",
    "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f55e2",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()`\n",
    "\n",
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, append a tuple (or a list) containing the user message and the LLM's response:\n",
    "`chatbot_object.append( (user_message, llm_message) )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43beebb7-40a6-4af5-a701-882821b6ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ.get('PORT2', 7870)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646d777-c211-4d31-9426-7b5d78b533ae",
   "metadata": {},
   "source": [
    "#### Format the prompt with the chat history\n",
    "\n",
    "- You can iterate through the chatbot object with a for loop.\n",
    "- Each item is a tuple containing the user message and the LLM's message.\n",
    "\n",
    "```Python\n",
    "for turn in chat_history:\n",
    "    user_msg, bot_msg = turn\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = client.generate(formatted_prompt,\n",
    "                                     max_new_tokens=1024,\n",
    "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ.get('PORT3', 7880)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b8de8",
   "metadata": {},
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e4fff81-a3d1-4cb8-8d6e-d152ab39065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee9bc5-fce7-44b1-af2a-e69bc7c598b6",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
    "- The `for` loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "700eb3bc-b63a-4ccb-94c4-70ec2e54bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                      max_new_tokens=1024,\n",
    "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
    "                                      temperature=temperature)\n",
    "                                      #stop_sequences to not generate the user answer\n",
    "    acc_text = \"\"\n",
    "    #Streaming the tokens\n",
    "    for idx, response in enumerate(stream):\n",
    "            text_token = response.token.text\n",
    "\n",
    "            if response.details:\n",
    "                return\n",
    "\n",
    "            if idx == 0 and text_token.startswith(\" \"):\n",
    "                text_token = text_token[1:]\n",
    "\n",
    "            acc_text += text_token\n",
    "            last_turn = list(chat_history.pop(-1))\n",
    "            last_turn[-1] += acc_text\n",
    "            chat_history = chat_history + [last_turn]\n",
    "            yield \"\", chat_history\n",
    "            acc_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09873dfd-5b6c-41d6-9479-12e8c8894295",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True, server_port=int(os.environ.get('PORT4', 7890)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a51a07",
   "metadata": {},
   "source": [
    "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`. \"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5276f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we built a chat application using Gradio and the Hugging Face API to interact with the `falcon-40b-instruct` model. We covered the setup of the environment, loading the model, and creating an interactive interface with advanced features. We hope this tutorial helps you understand how to integrate LLMs into your applications.\n",
    "\n",
    "For further reading and exploration:\n",
    "- [Gradio Documentation](https://gradio.app)\n",
    "- [Hugging Face API](https://huggingface.co/docs)\n",
    "- [Open LLM Leaderboard](https://huggingface.co/open-llm-leaderboard)\n",
    "\n",
    "Feel free to experiment with different models and customize the application to suit your needs. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
