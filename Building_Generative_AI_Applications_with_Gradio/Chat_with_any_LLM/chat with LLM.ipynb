{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5753a0c",
   "metadata": {},
   "source": [
    "# Chat with any LLM! ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d115b64",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build an application to chat with any Large Language Model (LLM) using Gradio. We will leverage the [Hugging Face API](https://huggingface.co) to interact with the `falcon-40b-instruct` model, which is one of the top-ranking open-source LLMs. By the end of this notebook, you will have a functional chat application that can interact with users using natural language.\n",
    "\n",
    "- Key steps include:\n",
    "1. Setting up the environment and installing necessary packages.\n",
    "2. Loading the LLM and configuring the API.\n",
    "3. Building the Gradio interface for user interaction.\n",
    "4. Adding advanced features like chat history and customized prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b5629",
   "metadata": {},
   "source": [
    "### Setting up the environment and installing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install python-dotenv text-generation gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50593af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import requests\n",
    "import warnings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "from text_generation import Client\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "hf_api_key = os.environ.get('HF_API_KEY')\n",
    "hf_api_falcom_base = os.environ.get('HF_API_FALCOM_BASE')\n",
    "\n",
    "# Uncomment the following line to print HF API Key and hf_api_falcom_base\n",
    "print(\"HF API Key:\", hf_api_key)\n",
    "print(\"Endpoint URL:\", hf_api_falcom_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfccb1",
   "metadata": {},
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79",
   "metadata": {},
   "source": [
    "import os # Provides a way of using operating system-dependent functionality\n",
    "import io # Provides core tools for working with streams of data\n",
    "import IPython.display\n",
    "from IPython.display import Image, display, HTML  # Used for displaying rich content (e.g., images, HTML) in Jupyter Notebooks\n",
    "from PIL import Image # Python Imaging Library for opening, manipulating, and saving image files\n",
    "import base64  # Encodes and decodes data in base64 format\n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "#print(hf_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411fae46",
   "metadata": {},
   "source": [
    "To configure the text-generation model, the correct endpoint (HF_API_FALCOM_BASE) must be selected, connecting the application to the desired model. Initially, the falcon-40b-instruct model [Inference Endpoint](https://huggingface.co/inference-endpoints) was suggested, but its Inference API has been deactivated. As a result, the tiiuae/falcon-7b-instruct model was choosen due to its similar capabilities, efficient performance, and lower resource requirements. This model remains effective for text-generation tasks, making it a practical choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation client\n",
    "client = Client(hf_api_falcom_base, headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d854e7",
   "metadata": {},
   "source": [
    "Suggestion from Github copilot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127813b",
   "metadata": {},
   "source": [
    "# Define the chat function\n",
    "def generate(input, max_new_tokens):\n",
    "    try:\n",
    "        response = client.generate(input, max_new_tokens=max_new_tokens)\n",
    "        return response.generated_text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "        gr.Slider(label=\"Max new tokens\", value=20, maximum=1024, minimum=1)\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Completion\")\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True, server_port=int(os.environ.get('PORT1', 7860)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c533ef",
   "metadata": {},
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_falcom_base=os.environ['HF_API_FALCOM_BASE']\n",
    "#print(hf_api_falcom_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd878694",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ed5b7",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import os\n",
    "from text_generation import Client\n",
    "import warnings\n",
    "\n",
    "# Suppress the warning (if functionality is unaffected)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")\n",
    "\n",
    "#FalcomLM-instruct endpoint on the text_generation library\n",
    "#client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)\n",
    "client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Bearer {os.environ['HF_API_KEY']}\"}, timeout=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6fc97",
   "metadata": {},
   "source": [
    "## Building an app to chat with any LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a3c9b",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , the best ranking open source LLM on the [ðŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Nowdays, the `falcon-40b-instruct` Inference API has been deactivated. As a result, the tiiuae/falcon-7b-instruct model was choosen due to its similar capabilities, efficient performance, and lower resource requirements. This model remains effective for text-generation tasks, making it a practical choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7065860-3c0b-490d-9e7c-22e5b79fc004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMath has been discovered, not invented. It is a system of rules and formulas that are used to describe the natural world and its behavior.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "client.generate(prompt, max_new_tokens=256).generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff84d4",
   "metadata": {},
   "source": [
    "The snippet below code does give the same response as the previous code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c32879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_text='\\nMath has been discovered, not invented. It is a system of rules and formulas that are used to describe the natural world and its behavior.' details=Details(finish_reason=<FinishReason.EndOfSequenceToken: 'eos_token'>, generated_tokens=30, seed=None, prefill=[], tokens=[Token(id=193, text='\\n', logprob=-0.004764557, special=False), Token(id=25864, text='Math', logprob=-0.19885254, special=False), Token(id=504, text=' has', logprob=-0.3696289, special=False), Token(id=650, text=' been', logprob=-0.6875, special=False), Token(id=6524, text=' discovered', logprob=-0.5136719, special=False), Token(id=23, text=',', logprob=-1.4863281, special=False), Token(id=416, text=' not', logprob=-0.79785156, special=False), Token(id=21886, text=' invented', logprob=-0.0031585693, special=False), Token(id=25, text='.', logprob=-0.09484863, special=False), Token(id=605, text=' It', logprob=-1.2431641, special=False), Token(id=304, text=' is', logprob=-0.93408203, special=False), Token(id=241, text=' a', logprob=-0.7324219, special=False), Token(id=1092, text=' system', logprob=-1.8076172, special=False), Token(id=275, text=' of', logprob=-0.08630371, special=False), Token(id=4213, text=' rules', logprob=-1.8759766, special=False), Token(id=273, text=' and', logprob=-0.48168945, special=False), Token(id=29418, text=' formulas', logprob=-1.7167969, special=False), Token(id=325, text=' that', logprob=-0.47973633, special=False), Token(id=362, text=' are', logprob=-1.4433594, special=False), Token(id=1042, text=' used', logprob=-0.2626953, special=False), Token(id=271, text=' to', logprob=-0.13977051, special=False), Token(id=7474, text=' describe', logprob=-1.5380859, special=False), Token(id=248, text=' the', logprob=-0.6147461, special=False), Token(id=2616, text=' natural', logprob=-0.8886719, special=False), Token(id=1079, text=' world', logprob=-0.053009033, special=False), Token(id=273, text=' and', logprob=-0.5957031, special=False), Token(id=701, text=' its', logprob=-0.8833008, special=False), Token(id=5188, text=' behavior', logprob=-1.6728516, special=False), Token(id=25, text='.', logprob=-0.16625977, special=False), Token(id=11, text='<|endoftext|>', logprob=-1.4736328, special=True)], top_tokens=None, best_of_sequences=None)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "response=client.generate(prompt, max_new_tokens=256)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cf162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the port1 from the environment\n",
    "PORT1 = int(os.environ.get('PORT1', 7860))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6d363",
   "metadata": {},
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c1a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from text_generation import Client\n",
    "import warnings\n",
    "\n",
    "# Suppress the warning (if functionality is unaffected)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")\n",
    "\n",
    "#FalcomLM-instruct endpoint on the text_generation library\n",
    "#client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Bearer {hf_api_key}\"}, timeout=120)\n",
    "client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Bearer {os.environ['HF_API_KEY']}\"}, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://55757c08942bb4308e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://55757c08942bb4308e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Back to Lesson 2, time flies!\n",
    "import gradio as gr\n",
    "def generate(input, slider):\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "                            gr.Slider(label=\"Max new tokens\", \n",
    "                                      value=20,  \n",
    "                                      maximum=1024, \n",
    "                                      minimum=1)], \n",
    "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f55e2",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()`\n",
    "\n",
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, append a tuple (or a list) containing the user message and the LLM's response:\n",
    "`chatbot_object.append( (user_message, llm_message) )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d5f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the port2 from the environment\n",
    "PORT2 = int(os.environ.get('PORT2', 7870))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43beebb7-40a6-4af5-a701-882821b6ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://4cfec70edddb0bb6fb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4cfec70edddb0bb6fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646d777-c211-4d31-9426-7b5d78b533ae",
   "metadata": {},
   "source": [
    "#### Format the prompt with the chat history\n",
    "\n",
    "- You can iterate through the chatbot object with a for loop.\n",
    "- Each item is a tuple containing the user message and the LLM's message.\n",
    "\n",
    "```Python\n",
    "for turn in chat_history:\n",
    "    user_msg, bot_msg = turn\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a21497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the port3 from the environment\n",
    "PORT3 = int(os.environ.get('PORT3', 7880))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "* Running on public URL: https://f7bb0a63ad89acf235.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f7bb0a63ad89acf235.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = client.generate(formatted_prompt,\n",
    "                                     max_new_tokens=1024,\n",
    "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b8de8",
   "metadata": {},
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4fff81-a3d1-4cb8-8d6e-d152ab39065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee9bc5-fce7-44b1-af2a-e69bc7c598b6",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
    "- The `for` loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f72110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the port4 from the environment\n",
    "PORT4 = int(os.environ.get('PORT4', 7890))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700eb3bc-b63a-4ccb-94c4-70ec2e54bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                      max_new_tokens=1024,\n",
    "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
    "                                      temperature=temperature)\n",
    "                                      #stop_sequences to not generate the user answer\n",
    "    acc_text = \"\"\n",
    "    #Streaming the tokens\n",
    "    for idx, response in enumerate(stream):\n",
    "            text_token = response.token.text\n",
    "\n",
    "            if response.details:\n",
    "                return\n",
    "\n",
    "            if idx == 0 and text_token.startswith(\" \"):\n",
    "                text_token = text_token[1:]\n",
    "\n",
    "            acc_text += text_token\n",
    "            last_turn = list(chat_history.pop(-1))\n",
    "            last_turn[-1] += acc_text\n",
    "            chat_history = chat_history + [last_turn]\n",
    "            yield \"\", chat_history\n",
    "            acc_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09873dfd-5b6c-41d6-9479-12e8c8894295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "* Running on public URL: https://c38679986663c7f3da.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c38679986663c7f3da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a51a07",
   "metadata": {},
   "source": [
    "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`. \"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5276f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we built a chat application using Gradio and the Hugging Face API to interact with the `falcon-40b-instruct` model. We covered the setup of the environment, loading the model, and creating an interactive interface with advanced features. We hope this tutorial helps you understand how to integrate LLMs into your applications.\n",
    "\n",
    "For further reading and exploration:\n",
    "- [Gradio Documentation](https://gradio.app)\n",
    "- [Hugging Face API](https://huggingface.co/docs)\n",
    "- [Open LLM Leaderboard](https://huggingface.co/open-llm-leaderboard)\n",
    "\n",
    "Feel free to experiment with different models and customize the application to suit your needs. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
