{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88535ead",
   "metadata": {},
   "source": [
    "# L1: NLP tasks with a simple interface ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e3a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv #Python-dotenv is a library to load environment variables from a .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa43ba",
   "metadata": {},
   "source": [
    "## Load the HF API key and relevant Python libraries.\n",
    "- import os  # Provides a way of using operating system-dependent functionality\n",
    "- import io  # Provides core tools for working with streams of data\n",
    "- from IPython.display import Image, display, HTML  # Used for displaying rich content (e.g., images, HTML) in Jupyter Notebooks\n",
    "- from PIL import Image  # Python Imaging Library for opening, manipulating, and saving image files\n",
    "-import base64  # Encodes and decodes data in base64 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os # Provides a way of using operating system-dependent functionality\n",
    "import io  # Provides core tools for working with streams of data\n",
    "from IPython.display import Image, display, HTML # Used for displaying rich content (e.g., images, HTML) in Jupyter Notebooks\n",
    "from PIL import Image # Python Imaging Library for opening, manipulating, and saving image files\n",
    "import base64  # Encodes and decodes data in base64 format\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "print(hf_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64744803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api-inference.huggingface.co/models/facebook/bart-large-cnn\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']\n",
    "print(ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a106ab02-f248-4c03-9dd8-b1991db7f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "\n",
    "#Summarization endpoint\n",
    "def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): \n",
    "\n",
    "#def get_completion(inputs, ENDPOINT_URL, parameters=None):     \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = { \"inputs\": inputs }\n",
    "    if parameters is not None:\n",
    "        data.update({\"parameters\": parameters})\n",
    "    response = requests.request(\"POST\",\n",
    "                                ENDPOINT_URL, headers=headers,\n",
    "                                data=json.dumps(data)\n",
    "                               )\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01892d1a",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "The code would look very similar if you were running it locally instead of from an API. The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a06f9",
   "metadata": {},
   "source": [
    "## Building a text summarization app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. It is the second tallest free-standing structure in France after the Millau Viaduct.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
    "        as an 81-storey building, and the tallest structure in Paris. \n",
    "        Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "        During its construction, the Eiffel Tower surpassed the Washington \n",
    "        Monument to become the tallest man-made structure in the world,\n",
    "        a title it held for 41 years until the Chrysler Building\n",
    "        in New York City was finished in 1930. It was the first structure \n",
    "        to reach a height of 300 metres. Due to the addition of a broadcasting \n",
    "        aerial at the top of the tower in 1957, it is now taller than the \n",
    "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the \n",
    "        Eiffel Tower is the second tallest free-standing structure in France \n",
    "        after the Millau Viaduct.''')\n",
    "\n",
    "get_completion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02d79f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n",
      "building. Its base is square, measuring 125 metres (410 ft) on each side. It is\n",
      "the second tallest free-standing structure in France after the Millau Viaduct.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Assuming `get_completion` returns a list of dictionaries\n",
    "#output = [{'summary_text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. It is the second tallest free-standing structure in France after the Millau Viaduct.'}]\n",
    "output = get_completion(text)\n",
    "\n",
    "\n",
    "# Extract the summary text\n",
    "if output and 'summary_text' in output[0]:\n",
    "    summary = output[0]['summary_text']\n",
    "    # Wrap and print the text with proper formatting\n",
    "    formatted_text = textwrap.fill(summary, width=80)\n",
    "    print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44b1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary of the given text is:\n",
      "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n",
      "building. Its base is square, measuring 125 metres (410 ft) on each side. It is\n",
      "the second tallest free-standing structure in France after the Millau Viaduct.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The summary of the given text is:\\n{formatted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144593f",
   "metadata": {},
   "source": [
    "### Getting started with Gradio `gr.Interface` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758c4a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# Access the PORT1 variable\n",
    "PORT1 = int(os.environ.get('PORT1', 7860))  # Fallback to 7860 if PORT1 is not set\n",
    "print(f\"Running on port: {PORT1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b0d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (4.7.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.5.2 (from gradio)\n",
      "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (2.2.0)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (11.0.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.45.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/codespace/.local/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from gradio-client==1.5.2->gradio) (2024.2.0)\n",
      "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.2->gradio)\n",
      "  Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.25.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.25.1->gradio)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading orjson-3.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, websockets, tqdm, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, mdurl, markupsafe, ffmpy, click, annotated-types, aiofiles, uvicorn, starlette, pydantic, markdown-it-py, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 click-8.1.8 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 huggingface-hub-0.27.0 markdown-it-py-3.0.0 markupsafe-2.1.5 mdurl-0.1.2 orjson-3.10.13 pydantic-2.10.4 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 rich-13.9.4 ruff-0.8.5 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.41.3 tomlkit-0.13.2 tqdm-4.67.1 typer-0.15.1 uvicorn-0.34.0 websockets-14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb11460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://73eee4c78f5b38f261.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://73eee4c78f5b38f261.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f18519",
   "metadata": {},
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b310770",
   "metadata": {},
   "source": [
    "`demo.launch(share=True)` lets you create a public link to share with your team or friends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601afff",
   "metadata": {},
   "source": [
    " demo.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8ecb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48cfae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on port: 7870\n"
     ]
    }
   ],
   "source": [
    "# Access the PORT2 variable\n",
    "PORT2 = int(os.environ.get('PORT2', 7870))  # Fallback to 7860 if PORT1 is not set\n",
    "print(f\"Running on port: {PORT2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://4bb3ea3bcafa40c7fb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4bb3ea3bcafa40c7fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, \n",
    "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
    "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
    "                    title=\"Text summarization with distilbart-cnn\",\n",
    "                    description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\"\n",
    "                   )\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28e35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b300d17",
   "metadata": {},
   "source": [
    "## Building a Named Entity Recognition app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1043f",
   "metadata": {},
   "source": [
    "We are using this [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663dcbb",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e965c775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.8131653666496277,\n",
       "  'word': 'Michela',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9298146963119507,\n",
       "  'word': 'DeepLearningA',\n",
       "  'start': 38,\n",
       "  'end': 51},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9996592998504639,\n",
       "  'word': 'Italy',\n",
       "  'start': 67,\n",
       "  'end': 72}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
    "#text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
    "text = \"My name is Michela, I'm learning from DeepLearningAI and I live in Italy\"\n",
    "get_completion(text, parameters=None, ENDPOINT_URL= API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f60b3-bdf0-4b96-a387-7a48b9017ca7",
   "metadata": {},
   "source": [
    "#### gr.interface()\n",
    "- Notice below that we pass in a list `[]` to `inputs` and to `outputs` because the function `fn` (in this case, `ner()`, can take in more than one input and return more than one output.\n",
    "- The number of objects passed to `inputs` list should match the number of parameters that the `fn` function takes in, and the number of objects passed to the `outputs` list should match the number of objects returned by the `fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49de1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "* Running on public URL: https://e2d15b6e534b247ede.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e2d15b6e534b247ede.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 2057, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 1863, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/components/highlighted_text.py\", line 154, in postprocess\n",
      "    entities = sorted(entities, key=lambda x: x[\"start\"])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/components/highlighted_text.py\", line 154, in <lambda>\n",
      "    entities = sorted(entities, key=lambda x: x[\"start\"])\n",
      "                                              ~^^^^^^^^^\n",
      "TypeError: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
    "                    examples=[\"My name is Michela and I live in Italy\", \"My name is Andrew and work at HuggingFace\"])\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "109b6ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7880\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f16ad4",
   "metadata": {},
   "source": [
    "### Adding a helper function to merge tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "* Running on public URL: https://c13e098137be0134f6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c13e098137be0134f6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2621/726961222.py\", line 18, in ner\n",
      "    merged_tokens = merge_tokens(output)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2621/726961222.py\", line 4, in merge_tokens\n",
      "    if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
      "                         ~~~~~^^^^^^^^^^\n",
      "KeyError: 'entity'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2621/726961222.py\", line 18, in ner\n",
      "    merged_tokens = merge_tokens(output)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2621/726961222.py\", line 4, in merge_tokens\n",
      "    if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
      "                         ~~~~~^^^^^^^^^^\n",
      "KeyError: 'entity'\n"
     ]
    }
   ],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
    "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Michela, I live in Italy and learn from HuggingFace\"])\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6196e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    print(\"Debug output:\", output)  # Debugging step\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "042c3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if 'entity' not in token:\n",
    "            continue  # Skip tokens without 'entity'\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            merged_tokens.append(token)\n",
    "    return merged_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "611abe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    try:\n",
    "        output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "        merged_tokens = merge_tokens(output)\n",
    "        return {\"text\": input, \"entities\": merged_tokens}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\"text\": input, \"entities\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a801c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7880\n",
      "Closing server running on port: 7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "* Running on public URL: https://c36145b46381ba3a55.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c36145b46381ba3a55.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Mock function simulating an API response\n",
    "def get_completion(input, parameters=None, ENDPOINT_URL=None):\n",
    "    # Simulating tokens, some of which might be missing the 'entity' key\n",
    "    return [\n",
    "        {\"word\": \"My\", \"start\": 0, \"end\": 2, \"score\": 0.95, \"entity\": \"B-PER\"},\n",
    "        {\"word\": \"name\", \"start\": 3, \"end\": 7, \"score\": 0.98, \"entity\": \"I-PER\"},\n",
    "        {\"word\": \"is\", \"start\": 8, \"end\": 10, \"score\": 0.90},\n",
    "        {\"word\": \"Andrew\", \"start\": 11, \"end\": 17, \"score\": 0.99, \"entity\": \"I-PER\"},\n",
    "        {\"word\": \"and\", \"start\": 18, \"end\": 21, \"score\": 0.88},\n",
    "        {\"word\": \"I\", \"start\": 22, \"end\": 23, \"score\": 0.93, \"entity\": \"B-PER\"},\n",
    "        {\"word\": \"live\", \"start\": 24, \"end\": 28, \"score\": 0.97, \"entity\": \"O\"},\n",
    "        {\"word\": \"in\", \"start\": 29, \"end\": 31, \"score\": 0.89, \"entity\": \"O\"},\n",
    "        {\"word\": \"California\", \"start\": 32, \"end\": 42, \"score\": 0.96, \"entity\": \"B-LOC\"},\n",
    "    ]\n",
    "\n",
    "# Updated function to merge tokens\n",
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if 'entity' not in token:\n",
    "            continue  # Skip tokens without 'entity'\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            merged_tokens.append(token)\n",
    "    return merged_tokens\n",
    "\n",
    "# Main NER function\n",
    "def ner(input):\n",
    "    try:\n",
    "        output = get_completion(input, parameters=None, ENDPOINT_URL=None)\n",
    "        merged_tokens = merge_tokens(output)\n",
    "        return {\"text\": input, \"entities\": merged_tokens}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\"text\": input, \"entities\": []}\n",
    "\n",
    "# Gradio Interface\n",
    "gr.close_all()\n",
    "demo = gr.Interface(\n",
    "    fn=ner,\n",
    "    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "    title=\"NER with dslim/bert-base-NER\",\n",
    "    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "    allow_flagging=\"never\",\n",
    "    examples=[\n",
    "        \"My name is Andrew, I'm building DeeplearningAI and I live in California\",\n",
    "        \"My name is Michela, I live in Italy and learn from HuggingFace\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True, server_port=int(os.environ.get('PORT4', 7860)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Closing server running on port: 7880\n",
      "Closing server running on port: 7890\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7890\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5a409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
